# LoRA configuration
lora_parameters:
  rank: 16
  alpha: 32
  dropout: 0.05
  scale: 1.0
